{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# JSON ファイルのパス（必要に応じてパスを調整してください）\n",
    "json_path = \"dataset/test_split.json\"\n",
    "\n",
    "# 出力ディレクトリの作成（存在しない場合）\n",
    "os.makedirs(\"PLU\", exist_ok=True)\n",
    "\n",
    "# JSON ファイルを読み込み\n",
    "with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# \"[Determine Substrate]\" の例だけを抽出\n",
    "filtered = [entry for entry in data if entry[\"instruction\"] == \"[Determine Substrate]\"]\n",
    "\n",
    "# PLU/input.txt 用：instruction と input (ここでは \"Seq=<...>\" の部分) をそのまま出力\n",
    "with open(\"PLU/input.txt\", \"w\", encoding=\"utf-8\") as f_in:\n",
    "    for entry in filtered:\n",
    "        line = f'{entry[\"instruction\"]} {entry[\"input\"]}'\n",
    "        f_in.write(line + \"\\n\")\n",
    "\n",
    "# PLU/true.txt 用：instruction, input, output を1行にまとめて出力\n",
    "with open(\"PLU/true.txt\", \"w\", encoding=\"utf-8\") as f_true:\n",
    "    for entry in filtered:\n",
    "        line = f'{entry[\"instruction\"]} {entry[\"input\"]} {entry[\"output\"]}'\n",
    "        f_true.write(line + \"\\n\")\n",
    "\n",
    "print(\"PLU/input.txt と PLU/true.txt の出力が完了しました。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer, GenerationConfig\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# 生成設定（必要に応じて調整してください）\n",
    "generation_config = GenerationConfig(\n",
    "    temperature=0.2,\n",
    "    top_k=40,\n",
    "    top_p=0.9,\n",
    "    do_sample=True,\n",
    "    num_beams=1,\n",
    "    repetition_penalty=1.2,\n",
    "    max_new_tokens=900\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU チェック\n",
    "load_type = torch.bfloat16\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(0)\n",
    "else:\n",
    "    raise ValueError(\"No GPU available.\")\n",
    "\n",
    "# モデルパスの指定\n",
    "model_path = 'stored_output/checkpoint-2epoch'\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=load_type,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map='auto',\n",
    "    quantization_config=None\n",
    ")\n",
    "model.eval()\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = []\n",
    "# 入力ファイルと出力ファイルのパス\n",
    "input_file = \"PLU/input.txt\"\n",
    "output_file = \"PLU/pred.txt\"\n",
    "# 出力先ディレクトリが存在しない場合は作成\n",
    "os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "# 入力ファイルの読み込み\n",
    "with open(input_file, 'r', encoding='utf-8') as f:\n",
    "    examples = f.read().splitlines()\n",
    "\n",
    "# バッチサイズの設定\n",
    "batch_size = 8\n",
    "tokenizer.padding_side = 'left'\n",
    "\n",
    "print(\"Start generating predictions...\")\n",
    "# バッチ処理の実装\n",
    "for i in tqdm(range(0, len(examples), batch_size), desc=\"Processing batches\"):\n",
    "    # 現在のバッチを取得\n",
    "    batch_examples = examples[i:i + batch_size]\n",
    "    \n",
    "    # バッチ内の例をトークナイズ\n",
    "    batch_inputs = tokenizer(batch_examples, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    \n",
    "    # モデルによる生成\n",
    "    batch_outputs = model.generate(\n",
    "        input_ids=batch_inputs[\"input_ids\"].to(device),\n",
    "        attention_mask=batch_inputs['attention_mask'].to(device),\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        generation_config=generation_config\n",
    "    )\n",
    "    \n",
    "    # 生成結果のデコード\n",
    "    for j in range(len(batch_examples)):\n",
    "        output = tokenizer.decode(batch_outputs[j], skip_special_tokens=True)\n",
    "        output = output.replace(\"</s>\", \"\")\n",
    "        outputs.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 予測結果の書き出し\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"\\n\".join(outputs))\n",
    "\n",
    "print(\"All the outputs have been saved in\", output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "行数が一致しました: 10684\n",
      "Evaluation Metrics:\n",
      "Precision: 0.9866 (39186/39719)\n",
      "Recall: 0.9519 (39186/41164)\n",
      "F1-Score: 0.9690\n",
      "Jaccard-Similarity: 0.9398 (39186/41697)\n",
      "Exact-Match: 0.9583 (10239/10684)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def extract_set(line):\n",
    "    \"\"\"\n",
    "    文字列から \"Substrate=<...>\" 部分を抽出し、カンマ区切りで分割した要素の集合を返す。\n",
    "    例: \"[Determine Substrate] Seq=<...> Substrate=<chloride, hydron>\" -> {\"chloride\", \"hydron\"}\n",
    "    \"\"\"\n",
    "    match = re.search(r\"Substrate=<(.+?)>\", line)\n",
    "    if match:\n",
    "        content = match.group(1)\n",
    "        # カンマで分割し、前後の空白を除去して小文字に（必要に応じて正規化）\n",
    "        elements = {elem.strip().lower() for elem in content.split(\",\") if elem.strip()}\n",
    "        return elements\n",
    "    return set()\n",
    "\n",
    "# ファイルの読み込み\n",
    "with open(\"PLU/true.txt\", \"r\", encoding=\"utf-8\") as f_true, open(\"PLU/pred.txt\", \"r\", encoding=\"utf-8\") as f_pred:\n",
    "    true_lines = f_true.read().splitlines()\n",
    "    pred_lines = f_pred.read().splitlines()\n",
    "\n",
    "if len(true_lines) != len(pred_lines):\n",
    "    print(f\"行数が一致しません: true: {len(true_lines)}, pred: {len(pred_lines)}\")\n",
    "    # 行数が異なる場合、少ない方に合わせて計算する\n",
    "    N = min(len(true_lines), len(pred_lines))\n",
    "    true_lines = true_lines[:N]\n",
    "    pred_lines = pred_lines[:N]\n",
    "else:\n",
    "    print(f\"行数が一致しました: {len(true_lines)}\")\n",
    "    N = len(true_lines)\n",
    "\n",
    "# 指標用の累積変数\n",
    "total_intersection = 0 # 予測と真の集合の積集合の要素数 Σ|Fi ∩ F'i|\n",
    "total_pred_count = 0 # 予測の集合の要素数 Σ|F'i|\n",
    "total_true_count = 0 # 真の集合の要素数 Σ|Fi|\n",
    "total_union = 0 # 予測と真の集合の和集合の要素数 Σ|Fi ∪ F'i|\n",
    "exact_match_count = 0  # 予測と真の集合が正確一致する行数 Σ δ(Fi, F'i)\n",
    "matched_indices = [] # 予測と真の集合が正確一致する行のインデックス\n",
    "mismatched_indices = [] # 予測と真の集合が正確一致しない行のインデックス\n",
    "\n",
    "# 各行ごとに評価\n",
    "for i, (t_line, p_line) in enumerate(zip(true_lines, pred_lines)):\n",
    "    true_set = extract_set(t_line)\n",
    "    pred_set = extract_set(p_line)\n",
    "    \n",
    "    intersection = true_set.intersection(pred_set)\n",
    "    union = true_set.union(pred_set)\n",
    "    \n",
    "    total_intersection += len(intersection)\n",
    "    total_pred_count += len(pred_set)\n",
    "    total_true_count += len(true_set)\n",
    "    total_union += len(union)\n",
    "    \n",
    "    # 一致判定とインデックスの格納\n",
    "    if true_set == pred_set:\n",
    "        exact_match_count += 1\n",
    "        matched_indices.append(i)\n",
    "    else:\n",
    "        mismatched_indices.append(i)\n",
    "\n",
    "# 各指標の計算\n",
    "precision = total_intersection / total_pred_count if total_pred_count > 0 else 0 # 適合率 = Σ|Fi ∩ F'i| / Σ|F'i|\n",
    "recall = total_intersection / total_true_count if total_true_count > 0 else 0 # 再現率 = Σ|Fi ∩ F'i| / Σ|Fi|\n",
    "f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0 # F1スコア = 2 * (適合率 * 再現率) / (適合率 + 再現率)\n",
    "jaccard = total_intersection / total_union if total_union > 0 else 0 # Jaccard係数 = Σ|Fi ∩ F'i| / Σ|Fi ∪ F'i|\n",
    "exact_match = exact_match_count / N # 正確一致率 = Σ δ(Fi, F'i) / N\n",
    "\n",
    "print(\"Evaluation Metrics:\")\n",
    "print(f\"Precision: {precision:.4f} ({total_intersection}/{total_pred_count})\")\n",
    "print(f\"Recall: {recall:.4f} ({total_intersection}/{total_true_count})\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "print(f\"Jaccard-Similarity: {jaccard:.4f} ({total_intersection}/{total_union})\")\n",
    "print(f\"Exact-Match: {exact_match:.4f} ({exact_match_count}/{N})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不一致行のサンプルを表示（最初の10件）\n",
    "if len(mismatched_indices) > 0:\n",
    "    print(\"\\n=== 不一致行のサンプル (最初の10件) ===\")\n",
    "    for i, idx in enumerate(mismatched_indices[:10]):\n",
    "        print(f\"インデックス {idx}:\")\n",
    "        print(f\"  正解: {true_lines[idx]}\")\n",
    "        print(f\"  予測: {pred_lines[idx]}\")\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
